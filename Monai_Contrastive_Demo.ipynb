{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e27854c",
   "metadata": {},
   "source": [
    "Welcome to my contrastive learning demo using MONAI's modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b040f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "import monai.transforms as M\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from monai.data import Dataset, DataLoader, CacheDataset, PersistentDataset, SmartCacheDataset\n",
    "from monai_model import CompleteNet, CNNBackbone\n",
    "from monai_train import predict, train_one_step, train_one_epoch, train\n",
    "from monai_dataloader import custom_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b9605f",
   "metadata": {},
   "source": [
    "VERY IMPORTANT: Monai readers by default swap axis 0 and 1 after loading the array with ``reverse_indexing`` set to ``True`` (\"because the spatial axes definition for non-medical specific file formats is different from other common medical packages\"). Also, I set image_only=True so that I don't get metadata about the image file. Also, we have to remember to add a channel since MONAI functions expect that channel before image size(Using EnsureChannelFirst()), and Monai's LoadImage does not add this channel dimension for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a01995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two functions help load in the image and ensure it has the extra channel dimension.\n",
    "load_image_monai = M.LoadImage(reader='pilreader', image_only=True, reverse_indexing=False) \n",
    "ensureChannel = M.EnsureChannelFirst()\n",
    "\n",
    "# These are just some of the many MONAI transformations we can play with\n",
    "resizeCrop = M.ResizeWithPadOrCrop((50, 50))\n",
    "randWeightCrop = M.RandWeightedCrop((50,50))\n",
    "randSpatialCrop = M.RandSpatialCrop(roi_size=(200,200), random_size=False)\n",
    "randRotate = M.RandRotate(range_x = 0.5, prob = 1.0)\n",
    "resize = M.Resize((200, 200))\n",
    "normalize = M.NormalizeIntensity()\n",
    "\n",
    "# Here are the transformations I decided on using in my code\n",
    "composedTransform = M.Compose([randSpatialCrop, randRotate])\n",
    "identityTransform = M.Compose([resize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f90ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_image_monai('/Users/hairanliang/Downloads/NORMAL-6477461-4.jpeg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8bc18de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([496, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape # Here, we don't have the needed channel dimension. We will have to use Monai's EnsureChannelFirst later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06540a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function MetaTensor.type>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15d445",
   "metadata": {},
   "source": [
    "Positive to Monai's LoadImage: it gives me a tensor immediately! In my previous code, I would load in the image, but then have to convert to tensor using ToTensor(). This saves me some time, but I have to remember to add in the extra channel dimension with ensureChannel, since ToTensor() gave me the extra channel dimension in my original code. Below, you can see the difference: ToTensor(), which is used in my original code, provides that extra dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d1438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, augmentation=True):\n",
    "    if augmentation:\n",
    "        augmented_data = composedTransform(data) \n",
    "    else:\n",
    "        augmented_data = identityTransform(data)\n",
    "    return augmented_data # This should be a tensor\n",
    "    \n",
    "def load_image(image_link):\n",
    "    image = Image.open(image_link)\n",
    "    image.show()\n",
    "    \n",
    "def load_data(image_link):\n",
    "    image = Image.open(image_link)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b8f0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig = load_data('/Users/hairanliang/Downloads/NORMAL-6477461-4.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98533d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toTens = T.ToTensor()\n",
    "y_tens = toTens(y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0262aaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 496, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tens.shape # Here, we see the channel dimension that we need, since it comes from ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd0f0b",
   "metadata": {},
   "source": [
    "Now, I implement my new get_item, which will take care of getting one item from my image_list, augmenting it twice, and returning two tensors stacked on top of each other. The key thing is we have to add the ensureChannel to make sure we have the channel dimension. The code for augment_data stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b6c3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item(link, augmentation=True):\n",
    "    x = load_image_monai(link)\n",
    "    x = ensureChannel(x)\n",
    "    aug_x = augment_data(x, augmentation)\n",
    "    return aug_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e66f66",
   "metadata": {},
   "source": [
    "What makes MONAI different: They have the \"_transform\", which is a neat way to specify a transform, in my case my transform is kinda unique since I need two transforms, so I just did it manually and without doing it within \"_transform\" but I basically moved all my code from getitem to transform so that it could work (\n",
    "data_i = self.data[index] is important within the \"_transform\").\n",
    "\n",
    "There is a special thing with MONAI where getitem should not be indexing, and instead leaving it for transform.\n",
    "When I tried indexing within getitem, it would instead of getting the first link, it would get the first character of the first link,\n",
    "and this is likely due to transform being the one who is responsible for first retrieving the first data (link)\n",
    "whereas in pytorch, they assume getitem_ gets the first index of the data (monai uses transform to do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abcbed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, image_list, augmentation_mode=False):\n",
    "        self.data = image_list # data = image_list\n",
    "        self.transform = None\n",
    "        self.augmentation_mode = augmentation_mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _transform(self, index):\n",
    "        data_i = self.data[index]\n",
    "        if self.augmentation_mode == True:\n",
    "            aug_x1 = get_item(data_i, self.augmentation_mode)\n",
    "            aug_x2 = get_item(data_i, self.augmentation_mode)\n",
    "            aug_stack = torch.stack((aug_x1, aug_x2), dim=0)\n",
    "            return aug_stack\n",
    "        else:\n",
    "            aug_x = get_item(data_i, self.augmentation_mode)\n",
    "            return aug_x\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._transform(index)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf3f4f",
   "metadata": {},
   "source": [
    "Now I begin to define my links, datasets, and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d263296",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_links_1 = ['/Users/hairanliang/Downloads/NORMAL-6477461-4.jpeg', \n",
    "               '/Users/hairanliang/Downloads/NORMAL-3767173-12.jpeg']\n",
    "image_links_2 = ['/Users/hairanliang/Downloads/NORMAL-9453329-20.jpeg',\n",
    "              '/Users/hairanliang/Downloads/NORMAL-7021113-21.jpeg']\n",
    "\n",
    "batch_test = ['/Users/hairanliang/Downloads/NORMAL-6477461-4.jpeg', \n",
    "               '/Users/hairanliang/Downloads/NORMAL-3767173-12.jpeg', \n",
    "             '/Users/hairanliang/Downloads/NORMAL-9453329-20.jpeg',\n",
    "              '/Users/hairanliang/Downloads/NORMAL-7021113-21.jpeg',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac442e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OCTDataset(batch_test, True) # Remember, true means augmentation_mode is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add0ba71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b4d954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([[[[69.3208, 67.4907, 66.0000,  ..., 51.6992, 51.1892, 50.6792],\n",
       "          [68.8108, 68.5106, 66.0000,  ..., 50.3521, 51.1170, 51.8820],\n",
       "          [68.3008, 69.5306, 66.0000,  ..., 53.1686, 53.6786, 54.1885],\n",
       "          ...,\n",
       "          [14.0000, 14.0000, 14.0000,  ...,  7.3257,  2.7041,  2.0000],\n",
       "          [11.3913,  9.6064,  7.8215,  ...,  5.2858,  4.2341,  2.0000],\n",
       "          [ 7.6792,  8.1892,  8.6992,  ...,  3.2458,  5.7640,  2.0000]]],\n",
       "\n",
       "\n",
       "        [[[38.1337, 57.6485, 44.3408,  ..., 70.0793, 65.9265, 63.2725],\n",
       "          [39.2272, 47.9264, 51.2621,  ..., 63.2012, 64.1240, 65.5155],\n",
       "          [45.2256, 37.3139, 57.7191,  ..., 64.9756, 74.8823, 85.0335],\n",
       "          ...,\n",
       "          [27.2123, 17.9841,  9.0489,  ...,  9.9389,  5.5029, 27.8111],\n",
       "          [ 9.4684, 10.7520, 12.5977,  ...,  9.1016,  7.8101, 14.4302],\n",
       "          [12.4550, 13.9755, 15.3598,  ...,  8.1788,  9.9532,  5.6812]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d21fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[[[ 71.7970,  79.1828,  86.0196,  ...,  39.7430,  40.7276,  41.7122],\n",
      "          [ 70.4186,  77.6075,  84.8381,  ...,  43.7420,  43.3481,  42.9543],\n",
      "          [ 69.0401,  76.0320,  83.6565,  ...,  42.2188,  42.6127,  43.0065],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.5471,  20.9041,  17.1887],\n",
      "          [  2.3856,   3.3704,   4.3550,  ...,   1.5318,  16.1777,  18.7640],\n",
      "          [ 13.6937,  17.4354,  21.1768,  ...,   2.5163,  11.4514,  20.3395]]],\n",
      "\n",
      "\n",
      "        [[[ 66.3475,  70.3470,  73.7599,  ..., 160.5477, 166.0163, 165.3516],\n",
      "          [ 66.2842,  70.2837,  73.7124,  ..., 138.1484, 132.5853, 132.1580],\n",
      "          [ 65.2970,  68.1041,  69.8062,  ..., 137.9533, 127.2787, 127.4527],\n",
      "          ...,\n",
      "          [  2.0000,   2.0000,   7.7365,  ...,  13.8628,  19.0148,  12.9724],\n",
      "          [  1.5867,   1.5709,   5.4858,  ...,  11.4380,  21.2872,  16.5657],\n",
      "          [  0.5869,   0.5710,   3.8145,  ...,  11.2009,  21.2398,  16.8033]]]])\n",
      "metatensor([[[[ 19.4002,  18.9853,   7.4494,  ...,  31.1981,  30.3335,  29.4688],\n",
      "          [ 24.9342,  16.7371,  11.4270,  ...,  29.3812,  30.0730,  30.7647],\n",
      "          [ 30.4681,  14.4889,  15.4045,  ...,  34.9721,  36.5286,  38.0850],\n",
      "          ...,\n",
      "          [ 97.6761,  97.5032,  97.3302,  ...,  66.0186,  69.9641,  71.6396],\n",
      "          [ 95.7646,  95.0730,  94.3812,  ...,  64.9810,  69.4453,  71.4667],\n",
      "          [ 92.4125,  92.0666,  91.7208,  ...,  63.9433,  68.9265,  71.2938]]],\n",
      "\n",
      "\n",
      "        [[[  5.2035,   0.7199,   8.9689,  ...,  26.5280,  24.1002,  20.4778],\n",
      "          [  8.6405,   0.8396,   5.8110,  ...,  11.2716,   6.6589,   3.5887],\n",
      "          [ 14.4670,   2.2962,   4.1116,  ...,   1.6505,   0.6794,   0.0000],\n",
      "          ...,\n",
      "          [157.9693, 158.2080, 161.3640,  ..., 186.2378, 192.2098, 202.1750],\n",
      "          [167.6628, 173.6175, 181.6288,  ..., 185.5095, 189.5392, 199.9901],\n",
      "          [197.6187, 203.1000, 205.5278,  ..., 186.3858, 187.6915, 197.5396]]]])\n",
      "metatensor([[[[255.0000, 255.0000, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
      "          [255.0000, 255.0000, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
      "          [255.0000, 255.0000, 255.0000,  ..., 255.0000, 255.0000, 255.0000],\n",
      "          ...,\n",
      "          [119.7714, 117.5140, 115.2568,  ...,  87.9486,  91.2426,  95.2681],\n",
      "          [110.0296, 107.5494, 105.0663,  ...,  87.4972,  90.1139,  94.5909],\n",
      "          [ 99.3164,  97.4242,  95.6184,  ...,  87.0457,  88.9941,  93.8562]]],\n",
      "\n",
      "\n",
      "        [[[207.7154, 214.4954, 211.7837,  ..., 105.1927, 102.9767, 100.7606],\n",
      "          [205.7210, 213.3874, 214.2214,  ...,  95.5531,  93.7802,  92.0074],\n",
      "          [203.7265, 212.2793, 216.6591,  ...,  87.6590,  85.2214,  82.7837],\n",
      "          ...,\n",
      "          [ 15.4742,  15.2526,  15.0310,  ...,   8.2789,   0.4469,   0.0000],\n",
      "          [ 15.0000,  15.0000,  15.0000,  ...,  10.2735,   2.2197,   0.0000],\n",
      "          [ 16.4282,  17.0930,  17.7578,  ...,  12.2678,   3.9926,   0.0000]]]])\n",
      "metatensor([[[[21.9998, 23.3747, 22.7506,  ..., 21.1113, 20.6666, 21.6660],\n",
      "          [16.0003, 20.1250, 22.0833,  ..., 21.9581, 19.2917, 25.3324],\n",
      "          [16.8875, 12.1259, 16.2507,  ..., 20.5832, 21.8885, 28.9989],\n",
      "          ...,\n",
      "          [38.2499, 38.0000, 38.5277,  ..., 93.0555, 93.9720, 94.8887],\n",
      "          [38.0000, 38.0972, 38.9860,  ..., 90.1946, 92.1945, 93.1111],\n",
      "          [38.0000, 38.5555, 39.4443,  ..., 84.5014, 88.0422, 90.3337]]],\n",
      "\n",
      "\n",
      "        [[[28.3544, 21.9950, 16.0000,  ..., 25.6219, 26.9723, 27.0000],\n",
      "          [30.8801, 25.6976, 18.0098,  ..., 27.0000, 27.0000, 27.0000],\n",
      "          [25.0282, 28.3985, 22.0612,  ..., 27.0000, 27.0000, 27.0000],\n",
      "          ...,\n",
      "          [57.0000, 57.8931, 59.6937,  ..., 40.6734, 42.6992, 45.3781],\n",
      "          [59.6644, 61.9299, 65.5311,  ..., 40.2233, 41.3487, 44.0276],\n",
      "          [65.4723, 69.4601, 91.9668,  ..., 40.4537, 40.6661, 42.6772]]]])\n"
     ]
    }
   ],
   "source": [
    "# This is to test that my getitem/_transform is working within Dataset. This surprisingly was not trivial, until I\n",
    "# realized that _transform should be the one indexing, not getitem like I was used to from PyTorch's Dataset Class.\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee058a",
   "metadata": {},
   "source": [
    "I first test that my MONAI dataset is compatible with PyTorch's DataLoader, since it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2203671",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = torch.utils.data.DataLoader(dataset, 2, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5eacdea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c77e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "print(batch.shape) # The shape is what I expect. Batch size of 2, so 2 original images leads to 4 augmented images. \n",
    "# And, the 1 is there for the channel dimension, and the image is of dimension 200x200 after our transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98207f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 metatensor([[[[7.1167e+01, 7.5904e+01, 8.4355e+01,  ..., 5.0818e+01,\n",
      "           4.9859e+01, 4.8933e+01],\n",
      "          [7.2146e+01, 7.4306e+01, 8.1478e+01,  ..., 4.8317e+01,\n",
      "           4.7678e+01, 4.7038e+01],\n",
      "          [7.3424e+01, 7.2707e+01, 7.8600e+01,  ..., 4.6711e+01,\n",
      "           4.6391e+01, 4.6072e+01],\n",
      "          ...,\n",
      "          [2.4495e+01, 1.7781e+01, 1.1067e+01,  ..., 1.6445e+01,\n",
      "           1.5659e+01, 1.2969e+01],\n",
      "          [5.1343e+00, 7.3722e+00, 9.6103e+00,  ..., 1.8043e+01,\n",
      "           1.5339e+01, 1.4568e+01],\n",
      "          [1.1767e+01, 1.4005e+01, 1.6243e+01,  ..., 1.9642e+01,\n",
      "           1.5019e+01, 1.5967e+01]]],\n",
      "\n",
      "\n",
      "        [[[1.0700e+02, 1.0700e+02, 1.0700e+02,  ..., 8.4158e-01,\n",
      "           1.0328e+01, 5.0186e-01],\n",
      "          [1.1263e+02, 1.1158e+02, 1.1053e+02,  ..., 2.7671e+00,\n",
      "           8.4027e+00, 2.4272e+00],\n",
      "          [1.1023e+02, 1.1075e+02, 1.1128e+02,  ..., 4.6927e+00,\n",
      "           6.4770e+00, 4.3531e+00],\n",
      "          ...,\n",
      "          [1.1519e+01, 9.8883e+00, 1.8014e+01,  ..., 2.4427e+01,\n",
      "           2.4252e+01, 2.4076e+01],\n",
      "          [8.1927e+00, 1.1639e+01, 1.9239e+01,  ..., 2.4589e+01,\n",
      "           2.4764e+01, 2.4939e+01],\n",
      "          [4.8667e+00, 1.3389e+01, 2.0464e+01,  ..., 2.5979e+01,\n",
      "           2.5103e+01, 2.4228e+01]]],\n",
      "\n",
      "\n",
      "        [[[2.2106e+01, 1.7366e+01, 1.2992e+01,  ..., 3.8000e+01,\n",
      "           3.7489e+01, 3.8379e+01],\n",
      "          [2.3130e+01, 2.6922e+01, 2.6857e+01,  ..., 3.8000e+01,\n",
      "           3.7173e+01, 3.9327e+01],\n",
      "          [2.0941e+01, 1.8097e+01, 1.9329e+01,  ..., 3.7806e+01,\n",
      "           3.7428e+01, 4.0458e+01],\n",
      "          ...,\n",
      "          [2.5500e+02, 2.5500e+02, 2.5500e+02,  ..., 7.8357e+00,\n",
      "           9.1219e+00, 9.4379e+00],\n",
      "          [2.5500e+02, 2.5500e+02, 2.5500e+02,  ..., 2.5716e+00,\n",
      "           4.0391e+00, 5.9351e+00],\n",
      "          [2.5500e+02, 2.5500e+02, 2.5500e+02,  ..., 0.0000e+00,\n",
      "           6.7328e-01, 1.6213e+00]]],\n",
      "\n",
      "\n",
      "        [[[2.2718e+01, 3.8925e+01, 2.9919e+01,  ..., 4.4098e+00,\n",
      "           3.7588e+00, 3.1077e+00],\n",
      "          [2.7059e+01, 3.5018e+01, 3.2307e+01,  ..., 1.4813e+00,\n",
      "           8.3027e-01, 1.7923e-01],\n",
      "          [3.1399e+01, 3.1112e+01, 3.4694e+01,  ..., 0.0000e+00,\n",
      "           0.0000e+00, 0.0000e+00],\n",
      "          ...,\n",
      "          [4.2833e+01, 4.2399e+01, 4.1965e+01,  ..., 3.1729e+01,\n",
      "           3.7506e+01, 4.0760e+01],\n",
      "          [4.1060e+01, 4.1277e+01, 4.1494e+01,  ..., 2.8908e+01,\n",
      "           3.7723e+01, 3.9023e+01],\n",
      "          [4.2072e+01, 4.2506e+01, 4.2940e+01,  ..., 2.6086e+01,\n",
      "           3.7940e+01, 3.7287e+01]]]])\n",
      "1 metatensor([[[[ 25.3745,  18.9537,  21.3601,  ...,  36.5877,  36.8051,  37.0225],\n",
      "          [ 24.9760,  19.4971,  20.9616,  ...,  33.6244,  33.2983,  32.9722],\n",
      "          [ 24.5775,  20.0406,  20.5630,  ...,  31.7899,  31.8986,  32.0073],\n",
      "          ...,\n",
      "          [ 16.6545,  16.8357,  17.0169,  ...,  26.3697,  20.1946,  17.3918],\n",
      "          [ 21.9815,  22.1989,  22.4163,  ...,  26.6957,  20.2671,  17.5367],\n",
      "          [ 26.6592,  26.7316,  26.8041,  ...,  27.0219,  20.3395,  17.6816]]],\n",
      "\n",
      "\n",
      "        [[[  7.6646,  10.4918,  13.3188,  ...,  20.1208,  15.5676,  40.9276],\n",
      "          [  0.5925,   1.4852,   2.3780,  ...,  18.9304,  19.4365,  44.2012],\n",
      "          [  8.0111,   6.6719,   5.3328,  ...,  17.7402,  23.3050,  47.4749],\n",
      "          ...,\n",
      "          [ 71.3111,  70.8111,  73.7776,  ..., 184.4822, 182.8453, 181.2085],\n",
      "          [ 70.4184,  71.2574,  74.2240,  ..., 189.0183, 189.7624, 190.5063],\n",
      "          [ 69.5256,  71.7038,  74.6704,  ..., 178.6813, 181.5083, 184.3353]]],\n",
      "\n",
      "\n",
      "        [[[ 27.2520,  28.4292,  29.6063,  ..., 143.2360, 164.8606, 179.0000],\n",
      "          [ 25.3573,  25.6516,  25.9459,  ..., 150.0045, 171.3344, 179.0000],\n",
      "          [ 25.5984,  25.3041,  25.0099,  ..., 156.7729, 177.8086, 179.0000],\n",
      "          ...,\n",
      "          [  8.0984,   9.0000,   9.0000,  ...,   9.0000,   9.0000,   9.0000],\n",
      "          [  8.3927,   9.0000,   9.0000,  ...,   8.0542,   8.3484,   8.6427],\n",
      "          [  8.6870,   9.0000,   9.0000,  ...,   7.0984,   7.3927,   7.6870]]],\n",
      "\n",
      "\n",
      "        [[[ 58.7188,  58.0000,  60.5364,  ...,  94.7559,  94.2374,  93.7188],\n",
      "          [ 59.2373,  58.0000,  60.2252,  ...,  87.2092,  86.2758,  85.3423],\n",
      "          [ 59.7559,  58.0000,  59.9141,  ...,  78.2578,  77.3243,  76.3909],\n",
      "          ...,\n",
      "          [  2.0817,   2.8077,   3.5336,  ...,  13.6381,  13.7131,  14.6488],\n",
      "          [  8.5965,   9.0114,   9.4262,  ...,  13.7418,  13.5057,  14.7526],\n",
      "          [ 12.1437,  12.2475,  12.3512,  ...,  13.8455,  13.2982,  14.8563]]]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, samples in enumerate(data_train):\n",
    "    print(batch_idx, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d556df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.blocks import Convolution, MaxAvgPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c9362d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution(\n",
      "  (conv): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (adn): ADN(\n",
      "    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (D): Dropout(p=0.1, inplace=False)\n",
      "    (A): PReLU(num_parameters=1)\n",
      "  )\n",
      ")\n",
      "Convolution(\n",
      "  (conv): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (adn): ADN(\n",
      "    (N): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (D): Dropout(p=0.1, inplace=False)\n",
      "    (A): PReLU(num_parameters=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv1 = Convolution(\n",
    "    spatial_dims=2,\n",
    "    in_channels=1,\n",
    "    out_channels=64,\n",
    "    kernel_size = (5,5),\n",
    "    adn_ordering=\"NDA\",\n",
    "    act=(\"prelu\", {\"init\": 0.2}),\n",
    "    dropout=0.1\n",
    ")\n",
    "print(conv1)\n",
    "\n",
    "conv2 = Convolution(\n",
    "    spatial_dims=2,\n",
    "    in_channels=64,\n",
    "    out_channels=64,\n",
    "    kernel_size = (5,5),\n",
    "    adn_ordering=\"NDA\",\n",
    "    act=(\"prelu\", {\"init\": 0.2}),\n",
    "    dropout=0.1\n",
    ")\n",
    "print(conv2)\n",
    "\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "global_pool = nn.AdaptiveAvgPool2d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "786b931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CompleteNet(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(CompleteNet, self).__init__()\n",
    "        self.backbone = backbone # This is the CNN\n",
    "        self.fc1 = nn.Linear(64, 80) # Converting into linear layer\n",
    "        self.fc2 = nn.Linear(80, 40)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class CNNBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNBackbone, self).__init__()\n",
    "        self.conv1 = Convolution(\n",
    "                spatial_dims=2,\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size = (5,5),\n",
    "                adn_ordering=\"NDA\",\n",
    "                act=(\"prelu\", {\"init\": 0.2}),\n",
    "                dropout=0.1\n",
    "                )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv2 = Convolution(\n",
    "                spatial_dims=2,\n",
    "                in_channels=64,\n",
    "                out_channels=64,\n",
    "                kernel_size = (5,5),\n",
    "                adn_ordering=\"NDA\",\n",
    "                act=(\"prelu\", {\"init\": 0.2}),\n",
    "                dropout=0.1\n",
    "                )\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.global_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a723e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the test model \n",
    "\n",
    "backboneTest = CNNBackbone()\n",
    "modelTest = CompleteNet(backboneTest)\n",
    "learning_rate = 0.5\n",
    "optimizer = torch.optim.SGD(modelTest.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21dc7b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 200, 200])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5fa0c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor(1.0977, grad_fn=<AliasBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_step(samples, modelTest, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50560d06",
   "metadata": {},
   "source": [
    "Works for replacing with Monai Convolution Layer. Now I can try to do it with RESNET model, but those rely on changing other parameters as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7220eca",
   "metadata": {},
   "source": [
    "Seems like MONAI provides lots of nice customization within the Convolution models for instance. You have the option of. adding normalization, activation, dropout, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568847f",
   "metadata": {},
   "source": [
    "Current bugs of original framework: Training functions don't work if augmentation mode is False (which is fine, because we are never going to train if we have augmentation on False anyway). Something to do with shape within __getitem__ and custom_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9d26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaa1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
